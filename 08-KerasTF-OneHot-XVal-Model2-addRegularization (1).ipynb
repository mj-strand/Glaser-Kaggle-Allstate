{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.18.1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  (188318, 132)\n",
      "test:  (125546, 131)\n"
     ]
    }
   ],
   "source": [
    "#Load the training and test files\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "print('training: ', df_train.shape)\n",
    "print('test: ', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Capture column names in variables\n",
    "cols = df_train.columns\n",
    "cols_test = df_test.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n",
       "       'cat9',\n",
       "       ...\n",
       "       'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n",
       "       'cont13', 'cont14', 'loss'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate features and target within training set\n",
    "# Drop ID column from training data\n",
    "X_train = df_train\n",
    "y_train = X_train[cols[-1]]\n",
    "X_train = X_train[cols[1:-1]]\n",
    "\n",
    "# Drop ID column from test data\n",
    "X_test = df_test\n",
    "X_test = X_test[cols_test[1:]]\n",
    "\n",
    "# Reset column variables\n",
    "cols = X_train.columns\n",
    "cols_test = X_test.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First 116 features look to be categorical in nature; Ran into errors just attempting to one hot encode\n",
    "# training data, fitting model and applying to test set.  Test set has categories that are not in training set.\n",
    "\n",
    "# Need to one hot encode across training and test sets because some categorical data is unique to either the \n",
    "# training or test data sets\n",
    "\n",
    "# Variable to hold the list of labels for all category features in the train and test data\n",
    "#\n",
    "labels = []\n",
    "\n",
    "# Loop through all the data to find all unique categories in both test and training data and append \n",
    "# to list of labels\n",
    "for i in range(0,116):\n",
    "    train = X_train[cols[i]].unique()\n",
    "    test = X_test[cols[i]].unique()\n",
    "    labels.append(list(set(train) | set(test)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1176)\n",
      "(188318, 1190)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#One hot encode all categorical features - training set\n",
    "cats = []\n",
    "for i in range(0, 116):\n",
    "    # Standardize labels by label encoding category data from 0 to n_classes-1\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(X_train.iloc[:,i])\n",
    "    feature = feature.reshape(X_train.shape[0], 1)\n",
    "    # One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "# Make a 2D array from a list of 1D arrays\n",
    "encoded_cats = np.column_stack(cats)\n",
    "\n",
    "# Print the shape of the encoded data\n",
    "print(encoded_cats.shape)\n",
    "\n",
    "# Concatenate continuous features to newly encoded attributes \n",
    "X_train = np.concatenate((encoded_cats,X_train.iloc[:,116:].values),axis=1)\n",
    "del cats\n",
    "del feature\n",
    "\n",
    "del encoded_cats\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125546, 1176)\n",
      "(125546, 1190)\n"
     ]
    }
   ],
   "source": [
    "# As above...One hot encode all categorical features - test set\n",
    "cats = []\n",
    "for i in range(0, 116):\n",
    "    #Label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(X_test.iloc[:,i])\n",
    "    feature = feature.reshape(X_test.shape[0], 1)\n",
    "    #One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "# Make a 2D array from a list of 1D arrays\n",
    "encoded_cats = np.column_stack(cats)\n",
    "\n",
    "# Print the shape of the encoded data\n",
    "print(encoded_cats.shape)\n",
    "\n",
    "# Concatenate encoded attributes with continuous attributes\n",
    "X_test = np.concatenate((encoded_cats,X_test.iloc[:,116:].values),axis=1)\n",
    "del cats\n",
    "del feature\n",
    "\n",
    "del encoded_cats\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "r, c = X_train.shape\n",
    "\n",
    "# split into training & validation data\n",
    "from sklearn import cross_validation\n",
    "X_train, X_val, y_train, y_val = cross_validation.train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37664, 1190)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def FFNN_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400, input_dim = c, W_regularizer=l2(0.1), init = 'he_normal'))\n",
    "    model.add(SReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(200, W_regularizer=l2(0.1), init = 'he_normal'))\n",
    "    model.add(SReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, init = 'he_normal'))\n",
    "    \n",
    "    # Use Mean Absolute Error as loss function per Kaggle\n",
    "    model.compile(loss = 'mae', optimizer = 'adadelta')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "32s - loss: 1325.8625\n",
      "Epoch 2/35\n",
      "29s - loss: 1243.0040\n",
      "Epoch 3/35\n",
      "29s - loss: 1226.2859\n",
      "Epoch 4/35\n",
      "29s - loss: 1215.3820\n",
      "Epoch 5/35\n",
      "29s - loss: 1206.0604\n",
      "Epoch 6/35\n",
      "29s - loss: 1199.8269\n",
      "Epoch 7/35\n",
      "30s - loss: 1193.4209\n",
      "Epoch 8/35\n",
      "33s - loss: 1188.2144\n",
      "Epoch 9/35\n",
      "27s - loss: 1185.1932\n",
      "Epoch 10/35\n",
      "30s - loss: 1180.8084\n",
      "Epoch 11/35\n",
      "30s - loss: 1177.4782\n",
      "Epoch 12/35\n",
      "30s - loss: 1174.8147\n",
      "Epoch 13/35\n",
      "30s - loss: 1171.0135\n",
      "Epoch 14/35\n",
      "30s - loss: 1167.6923\n",
      "Epoch 15/35\n",
      "30s - loss: 1164.9393\n",
      "Epoch 16/35\n",
      "30s - loss: 1161.7153\n",
      "Epoch 17/35\n",
      "30s - loss: 1157.9290\n",
      "Epoch 18/35\n",
      "30s - loss: 1156.1660\n",
      "Epoch 19/35\n",
      "30s - loss: 1152.0563\n",
      "Epoch 20/35\n",
      "29s - loss: 1150.0706\n",
      "Epoch 21/35\n",
      "30s - loss: 1147.7119\n",
      "Epoch 22/35\n",
      "31s - loss: 1145.5323\n",
      "Epoch 23/35\n",
      "31s - loss: 1144.1343\n",
      "Epoch 24/35\n",
      "31s - loss: 1140.6247\n",
      "Epoch 25/35\n",
      "31s - loss: 1138.2110\n",
      "Epoch 26/35\n",
      "37s - loss: 1136.1244\n",
      "Epoch 27/35\n",
      "43s - loss: 1132.9514\n",
      "Epoch 28/35\n",
      "29s - loss: 1133.5916\n",
      "Epoch 29/35\n",
      "29s - loss: 1130.6008\n",
      "Epoch 30/35\n",
      "29s - loss: 1128.9856\n",
      "Epoch 31/35\n",
      "29s - loss: 1128.3429\n",
      "Epoch 32/35\n",
      "29s - loss: 1123.4058\n",
      "Epoch 33/35\n",
      "29s - loss: 1122.3951\n",
      "Epoch 34/35\n",
      "29s - loss: 1120.6216\n",
      "Epoch 35/35\n",
      "29s - loss: 1119.7624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', <keras.wrappers.scikit_learn.KerasRegressor object at 0x7fc1b3b9d128>)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Create scikit-learn Pipeline\n",
    "# 1. Perform standard scaling of continuous data features (won't affect categorical features)\n",
    "# 2. Fit model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=FFNN_model, nb_epoch=35, batch_size=40, verbose=2)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159.9875111092931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = pipeline.predict(X_val)\n",
    "\n",
    "result = mean_absolute_error(y_val, pred_val)\n",
    "result                      \n",
    "# Activation ..> val score\n",
    "\n",
    "# SRelU(10) -> 1173.5\n",
    "# SReLU(20) -> 1166.9\n",
    "# SReLU(23) -> 1167.8\n",
    "# SReLU(25) -> 1163.9 --> 1143\n",
    "# SRelu(30) -> 1166.4 ->> 1146\n",
    "# SReLU(60) -> 1173.0 --> 1155 (on kaggle)\n",
    "\n",
    "# Reg(0.01), Epoch(25) -> 1161.7\n",
    "# Reg(0.01), Epoch(30) -> 1164.6\n",
    "# Reg(0.10), Epoch(30) -> 1158.9 --> 1143\n",
    "# Reg(0.10), Epoch(35) 1118 -> 1159.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_targets = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../input/output.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b3f477884167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     })\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/output.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../input/output.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "output = pd.DataFrame({\n",
    "        \"ID\": df_test[\"id\"],\n",
    "        \"loss\": pred_targets[:]\n",
    "    })\n",
    "output.to_csv(\"../input/output.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
